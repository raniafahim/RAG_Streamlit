import streamlit as st
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFaceHub
from langchain.docstore.document import Document

# ğŸ“Œ Chargement de l'embedding et du vecteurstore
MODEL_NAME_EMBEDDER = "BAAI/bge-m3"
embedder = HuggingFaceEmbeddings(model_name=MODEL_NAME_EMBEDDER)

vectorstore = Chroma(
    embedding_function=embedder,
    persist_directory="./chroma_db"
)

st.set_page_config(page_title="ğŸ“š RAG Visualizer", layout="wide")
st.title("ğŸ” Visualisation des chunks sÃ©lectionnÃ©s")

# ğŸ‘‰ Extraction des numÃ©ros de dossier disponibles
# Si pas possible via vectorstore.get(), tu peux prÃ©extraire les num_dossier
# et les stocker dans un fichier .json ou .csv pour charger ici
try:
    # Hack pour rÃ©cupÃ©rer des dossiers (si accessible)
    all_docs = vectorstore.get()["documents"]
    all_numdossiers = sorted(set([d.metadata["num_dossier"] for d in all_docs]))
except Exception:
    st.error("Impossible de lire les numÃ©ros de dossier. Fournir un fichier externe.")
    all_numdossiers = []

# ğŸ›ï¸ Interface utilisateur
num_dossier = st.selectbox("NumÃ©ro de dossier :", all_numdossiers)
k = st.slider("Nombre de chunks Ã  afficher (k)", min_value=1, max_value=20, value=5)
question = st.text_input("â“ Poser une question")

# Bouton d'action
if st.button("ğŸ” Voir les chunks sÃ©lectionnÃ©s") and question and num_dossier:

    retriever = vectorstore.as_retriever(
        search_kwargs={"k": k, "filter": {"num_dossier": num_dossier}}
    )
    docs: list[Document] = retriever.get_relevant_documents(question)

    st.subheader("ğŸ“„ Chunks rÃ©cupÃ©rÃ©s (contexte utilisÃ©)")

    for i, doc in enumerate(docs):
        st.markdown(f"### ğŸ”¹ Chunk {i+1}")
        st.markdown(f"`ID:` {doc.metadata.get('id', 'N/A')}")
        st.markdown(f"`NumDossier:` {doc.metadata.get('num_dossier', 'N/A')}")
        st.markdown("---")
        st.write(doc.page_content)
        st.markdown("---")

    if st.checkbox("ğŸ§  GÃ©nÃ©rer une rÃ©ponse Ã  partir de ces chunks ?"):
        llm = HuggingFaceHub(repo_id="google/flan-t5-large")
        qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)
        response = qa_chain.run(question)
        st.success("RÃ©ponse gÃ©nÃ©rÃ©e :")
        st.write(response)
